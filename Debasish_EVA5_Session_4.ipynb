{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Debasish EVA5 - Session 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debasishsarangi88/S4/blob/master/Debasish_EVA5_Session_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn.Module is the base class for all neural networks. When we pass this here, \n",
        "#we are saying that 'Net' class will inherit the nn.Module class. In other words, all our classess should sub-class the nn.Module.\n",
        "\n",
        "class Net(nn.Module): \n",
        "    def __init__(self): #It is a dunder method meant for initializing the values of instance members whenever a class is instantiated \n",
        "        super(Net, self).__init__() # super() is a way that lets you avoid referrring to the base class explicitly. This way we are able to call a method from the base class which is otherwise overwritten.\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3 ,padding=1) #InputSize: 28x28x1   OutputSize: 28x28x32  RF: 3x3  (Size does not decrease since padding is used)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3  , padding=1)#InputSize: 28x28x32  OutputSize: 28x28x64  RF: 5x5  (Size does not decrease since padding is used) \n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #InputSize: 28x28x64  OutputSize: 14x14x64  RF: 10x10(It is just an assumption that maxpooling doubles the receptive field as of now) \n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1) #InputSize: 14x14x64  OutputSize: 14x14x64 RF: 14x14(Size does not decrease since padding is used)\n",
        "        self.conv4 = nn.Conv2d(64,128, 3 , padding=1) #InputSize: 14x14x64 OutputSize: 14x14x128 RF: 14x14(Size does not decrease since padding is used)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) #InputSize: 14x14x128 OutputSize: 7x7x256   RF: 28x28(It is just an assumption that maxpooling doubles the receptive field as of now)\n",
        "        self.conv5 = nn.Conv2d(128,128, 3)  #InputSize: 7x7x256   OutputSize: 5x5x128   RF: 30x30\n",
        "        self.conv6 = nn.Conv2d(128,128, 3) #InputSize: 5x5x128   OutputSize: 3x3x128  RF: 32x32   \n",
        "        self.conv7 = nn.Conv2d(128, 10, 3) #InputSize: 3x3x128  OutputSize: 1x1x10    RF: 34x34\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8348bde-44e5-493d-bdab-7ae2be520abc"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4           [-1, 64, 14, 14]          36,928\n",
            "            Conv2d-5          [-1, 128, 14, 14]          73,856\n",
            "         MaxPool2d-6            [-1, 128, 7, 7]               0\n",
            "            Conv2d-7            [-1, 128, 5, 5]         147,584\n",
            "            Conv2d-8            [-1, 128, 3, 3]         147,584\n",
            "            Conv2d-9             [-1, 10, 1, 1]          11,530\n",
            "================================================================\n",
            "Total params: 436,298\n",
            "Trainable params: 436,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.04\n",
            "Params size (MB): 1.66\n",
            "Estimated Total Size (MB): 2.71\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d418a0a9-9cb9-4ce8-ff8c-0a3214e7d26a"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.06, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 8):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "\n",
            "loss=2.300487995147705 batch_id=0:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "loss=2.300487995147705 batch_id=0:   0%|          | 1/469 [00:00<00:52,  8.93it/s]\u001b[A\n",
            "loss=2.3048300743103027 batch_id=1:   0%|          | 1/469 [00:00<00:52,  8.93it/s]\u001b[A\n",
            "loss=2.305330276489258 batch_id=2:   0%|          | 1/469 [00:00<00:52,  8.93it/s] \u001b[A\n",
            "loss=2.305330276489258 batch_id=2:   1%|          | 3/469 [00:00<00:43, 10.68it/s]\u001b[A\n",
            "loss=2.301225185394287 batch_id=3:   1%|          | 3/469 [00:00<00:43, 10.68it/s]\u001b[A\n",
            "loss=2.304037094116211 batch_id=4:   1%|          | 3/469 [00:00<00:43, 10.68it/s]\u001b[A\n",
            "loss=2.304037094116211 batch_id=4:   1%|          | 5/469 [00:00<00:37, 12.28it/s]\u001b[A\n",
            "loss=2.3042821884155273 batch_id=5:   1%|          | 5/469 [00:00<00:37, 12.28it/s]\u001b[A\n",
            "loss=2.29647159576416 batch_id=6:   1%|          | 5/469 [00:00<00:37, 12.28it/s]  \u001b[A\n",
            "loss=2.29647159576416 batch_id=6:   1%|▏         | 7/469 [00:00<00:33, 13.67it/s]\u001b[A\n",
            "loss=2.296635150909424 batch_id=7:   1%|▏         | 7/469 [00:00<00:33, 13.67it/s]\u001b[A\n",
            "loss=2.2931551933288574 batch_id=8:   1%|▏         | 7/469 [00:00<00:33, 13.67it/s]\u001b[A\n",
            "loss=2.2931551933288574 batch_id=8:   2%|▏         | 9/469 [00:00<00:30, 14.91it/s]\u001b[A\n",
            "loss=2.290195941925049 batch_id=9:   2%|▏         | 9/469 [00:00<00:30, 14.91it/s] \u001b[A\n",
            "loss=2.292943000793457 batch_id=10:   2%|▏         | 9/469 [00:00<00:30, 14.91it/s]\u001b[A\n",
            "loss=2.292943000793457 batch_id=10:   2%|▏         | 11/469 [00:00<00:28, 16.01it/s]\u001b[A\n",
            "loss=2.293484687805176 batch_id=11:   2%|▏         | 11/469 [00:00<00:28, 16.01it/s]\u001b[A\n",
            "loss=2.289767026901245 batch_id=12:   2%|▏         | 11/469 [00:00<00:28, 16.01it/s]\u001b[A\n",
            "loss=2.289767026901245 batch_id=12:   3%|▎         | 13/469 [00:00<00:28, 16.20it/s]\u001b[A\n",
            "loss=2.2836477756500244 batch_id=13:   3%|▎         | 13/469 [00:00<00:28, 16.20it/s]\u001b[A\n",
            "loss=2.2859883308410645 batch_id=14:   3%|▎         | 13/469 [00:00<00:28, 16.20it/s]\u001b[A\n",
            "loss=2.2859883308410645 batch_id=14:   3%|▎         | 15/469 [00:00<00:27, 16.28it/s]\u001b[A\n",
            "loss=2.2776825428009033 batch_id=15:   3%|▎         | 15/469 [00:00<00:27, 16.28it/s]\u001b[A\n",
            "loss=2.276714324951172 batch_id=16:   3%|▎         | 15/469 [00:00<00:27, 16.28it/s] \u001b[A\n",
            "loss=2.276714324951172 batch_id=16:   4%|▎         | 17/469 [00:00<00:27, 16.69it/s]\u001b[A\n",
            "loss=2.263292074203491 batch_id=17:   4%|▎         | 17/469 [00:01<00:27, 16.69it/s]\u001b[A\n",
            "loss=2.2586357593536377 batch_id=18:   4%|▎         | 17/469 [00:01<00:27, 16.69it/s]\u001b[A\n",
            "loss=2.2586357593536377 batch_id=18:   4%|▍         | 19/469 [00:01<00:26, 17.20it/s]\u001b[A\n",
            "loss=2.2214112281799316 batch_id=19:   4%|▍         | 19/469 [00:01<00:26, 17.20it/s]\u001b[A\n",
            "loss=2.198622226715088 batch_id=20:   4%|▍         | 19/469 [00:01<00:26, 17.20it/s] \u001b[A\n",
            "loss=2.1065292358398438 batch_id=21:   4%|▍         | 19/469 [00:01<00:26, 17.20it/s]\u001b[A\n",
            "loss=2.1065292358398438 batch_id=21:   5%|▍         | 22/469 [00:01<00:24, 18.01it/s]\u001b[A\n",
            "loss=1.9735593795776367 batch_id=22:   5%|▍         | 22/469 [00:01<00:24, 18.01it/s]\u001b[A\n",
            "loss=1.6832228899002075 batch_id=23:   5%|▍         | 22/469 [00:01<00:24, 18.01it/s]\u001b[A\n",
            "loss=1.6832228899002075 batch_id=23:   5%|▌         | 24/469 [00:01<00:24, 18.53it/s]\u001b[A\n",
            "loss=2.2677879333496094 batch_id=24:   5%|▌         | 24/469 [00:01<00:24, 18.53it/s]\u001b[A\n",
            "loss=2.9722771644592285 batch_id=25:   5%|▌         | 24/469 [00:01<00:24, 18.53it/s]\u001b[A\n",
            "loss=2.27217173576355 batch_id=26:   5%|▌         | 24/469 [00:01<00:24, 18.53it/s]  \u001b[A\n",
            "loss=2.27217173576355 batch_id=26:   6%|▌         | 27/469 [00:01<00:23, 18.74it/s]\u001b[A\n",
            "loss=2.2954466342926025 batch_id=27:   6%|▌         | 27/469 [00:01<00:23, 18.74it/s]\u001b[A\n",
            "loss=2.3069303035736084 batch_id=28:   6%|▌         | 27/469 [00:01<00:23, 18.74it/s]\u001b[A\n",
            "loss=2.3069303035736084 batch_id=28:   6%|▌         | 29/469 [00:01<00:23, 18.81it/s]\u001b[A\n",
            "loss=2.3152689933776855 batch_id=29:   6%|▌         | 29/469 [00:01<00:23, 18.81it/s]\u001b[A\n",
            "loss=2.2834787368774414 batch_id=30:   6%|▌         | 29/469 [00:01<00:23, 18.81it/s]\u001b[A\n",
            "loss=2.2834787368774414 batch_id=30:   7%|▋         | 31/469 [00:01<00:23, 18.28it/s]\u001b[A\n",
            "loss=2.284219264984131 batch_id=31:   7%|▋         | 31/469 [00:01<00:23, 18.28it/s] \u001b[A\n",
            "loss=2.296255111694336 batch_id=32:   7%|▋         | 31/469 [00:01<00:23, 18.28it/s]\u001b[A\n",
            "loss=2.2864701747894287 batch_id=33:   7%|▋         | 31/469 [00:01<00:23, 18.28it/s]\u001b[A\n",
            "loss=2.2864701747894287 batch_id=33:   7%|▋         | 34/469 [00:01<00:22, 19.00it/s]\u001b[A\n",
            "loss=2.287222146987915 batch_id=34:   7%|▋         | 34/469 [00:01<00:22, 19.00it/s] \u001b[A\n",
            "loss=2.281324863433838 batch_id=35:   7%|▋         | 34/469 [00:01<00:22, 19.00it/s]\u001b[A\n",
            "loss=2.281324863433838 batch_id=35:   8%|▊         | 36/469 [00:01<00:22, 18.96it/s]\u001b[A\n",
            "loss=2.258333206176758 batch_id=36:   8%|▊         | 36/469 [00:02<00:22, 18.96it/s]\u001b[A\n",
            "loss=2.29308819770813 batch_id=37:   8%|▊         | 36/469 [00:02<00:22, 18.96it/s] \u001b[A\n",
            "loss=2.29308819770813 batch_id=37:   8%|▊         | 38/469 [00:02<00:22, 18.77it/s]\u001b[A\n",
            "loss=2.2971537113189697 batch_id=38:   8%|▊         | 38/469 [00:02<00:22, 18.77it/s]\u001b[A\n",
            "loss=2.234189510345459 batch_id=39:   8%|▊         | 38/469 [00:02<00:22, 18.77it/s] \u001b[A\n",
            "loss=2.234189510345459 batch_id=39:   9%|▊         | 40/469 [00:02<00:22, 18.87it/s]\u001b[A\n",
            "loss=2.2550554275512695 batch_id=40:   9%|▊         | 40/469 [00:02<00:22, 18.87it/s]\u001b[A\n",
            "loss=2.255722999572754 batch_id=41:   9%|▊         | 40/469 [00:02<00:22, 18.87it/s] \u001b[A\n",
            "loss=2.255722999572754 batch_id=41:   9%|▉         | 42/469 [00:02<00:22, 19.12it/s]\u001b[A\n",
            "loss=2.1551918983459473 batch_id=42:   9%|▉         | 42/469 [00:02<00:22, 19.12it/s]\u001b[A\n",
            "loss=2.175786018371582 batch_id=43:   9%|▉         | 42/469 [00:02<00:22, 19.12it/s] \u001b[A\n",
            "loss=2.175786018371582 batch_id=43:   9%|▉         | 44/469 [00:02<00:22, 19.22it/s]\u001b[A\n",
            "loss=2.066596508026123 batch_id=44:   9%|▉         | 44/469 [00:02<00:22, 19.22it/s]\u001b[A\n",
            "loss=2.0873849391937256 batch_id=45:   9%|▉         | 44/469 [00:02<00:22, 19.22it/s]\u001b[A\n",
            "loss=2.0873849391937256 batch_id=45:  10%|▉         | 46/469 [00:02<00:23, 18.21it/s]\u001b[A\n",
            "loss=2.185344696044922 batch_id=46:  10%|▉         | 46/469 [00:02<00:23, 18.21it/s] \u001b[A\n",
            "loss=1.966601848602295 batch_id=47:  10%|▉         | 46/469 [00:02<00:23, 18.21it/s]\u001b[A\n",
            "loss=1.8646317720413208 batch_id=48:  10%|▉         | 46/469 [00:02<00:23, 18.21it/s]\u001b[A\n",
            "loss=1.8646317720413208 batch_id=48:  10%|█         | 49/469 [00:02<00:22, 18.99it/s]\u001b[A\n",
            "loss=1.772743582725525 batch_id=49:  10%|█         | 49/469 [00:02<00:22, 18.99it/s] \u001b[A\n",
            "loss=1.6300488710403442 batch_id=50:  10%|█         | 49/469 [00:02<00:22, 18.99it/s]\u001b[A\n",
            "loss=1.6300488710403442 batch_id=50:  11%|█         | 51/469 [00:02<00:22, 18.60it/s]\u001b[A\n",
            "loss=4.5797271728515625 batch_id=51:  11%|█         | 51/469 [00:02<00:22, 18.60it/s]\u001b[A\n",
            "loss=2.225585699081421 batch_id=52:  11%|█         | 51/469 [00:02<00:22, 18.60it/s] \u001b[A\n",
            "loss=2.225585699081421 batch_id=52:  11%|█▏        | 53/469 [00:02<00:22, 18.67it/s]\u001b[A\n",
            "loss=2.2891592979431152 batch_id=53:  11%|█▏        | 53/469 [00:02<00:22, 18.67it/s]\u001b[A\n",
            "loss=2.296659231185913 batch_id=54:  11%|█▏        | 53/469 [00:02<00:22, 18.67it/s] \u001b[A\n",
            "loss=2.296659231185913 batch_id=54:  12%|█▏        | 55/469 [00:02<00:22, 18.51it/s]\u001b[A\n",
            "loss=2.279789447784424 batch_id=55:  12%|█▏        | 55/469 [00:03<00:22, 18.51it/s]\u001b[A\n",
            "loss=2.3051116466522217 batch_id=56:  12%|█▏        | 55/469 [00:03<00:22, 18.51it/s]\u001b[A\n",
            "loss=2.3051116466522217 batch_id=56:  12%|█▏        | 57/469 [00:03<00:23, 17.84it/s]\u001b[A\n",
            "loss=2.2690463066101074 batch_id=57:  12%|█▏        | 57/469 [00:03<00:23, 17.84it/s]\u001b[A\n",
            "loss=2.267179489135742 batch_id=58:  12%|█▏        | 57/469 [00:03<00:23, 17.84it/s] \u001b[A\n",
            "loss=2.267179489135742 batch_id=58:  13%|█▎        | 59/469 [00:03<00:23, 17.18it/s]\u001b[A\n",
            "loss=2.2390213012695312 batch_id=59:  13%|█▎        | 59/469 [00:03<00:23, 17.18it/s]\u001b[A\n",
            "loss=2.229261636734009 batch_id=60:  13%|█▎        | 59/469 [00:03<00:23, 17.18it/s] \u001b[A\n",
            "loss=2.229261636734009 batch_id=60:  13%|█▎        | 61/469 [00:03<00:23, 17.42it/s]\u001b[A\n",
            "loss=2.1971938610076904 batch_id=61:  13%|█▎        | 61/469 [00:03<00:23, 17.42it/s]\u001b[A\n",
            "loss=2.144019842147827 batch_id=62:  13%|█▎        | 61/469 [00:03<00:23, 17.42it/s] \u001b[A\n",
            "loss=2.144019842147827 batch_id=62:  13%|█▎        | 63/469 [00:03<00:23, 17.50it/s]\u001b[A\n",
            "loss=2.1399621963500977 batch_id=63:  13%|█▎        | 63/469 [00:03<00:23, 17.50it/s]\u001b[A\n",
            "loss=2.114591360092163 batch_id=64:  13%|█▎        | 63/469 [00:03<00:23, 17.50it/s] \u001b[A\n",
            "loss=2.114591360092163 batch_id=64:  14%|█▍        | 65/469 [00:03<00:22, 17.86it/s]\u001b[A\n",
            "loss=1.9396779537200928 batch_id=65:  14%|█▍        | 65/469 [00:03<00:22, 17.86it/s]\u001b[A\n",
            "loss=1.8371442556381226 batch_id=66:  14%|█▍        | 65/469 [00:03<00:22, 17.86it/s]\u001b[A\n",
            "loss=1.8371442556381226 batch_id=66:  14%|█▍        | 67/469 [00:03<00:22, 17.85it/s]\u001b[A\n",
            "loss=1.9576799869537354 batch_id=67:  14%|█▍        | 67/469 [00:03<00:22, 17.85it/s]\u001b[A\n",
            "loss=1.8012057542800903 batch_id=68:  14%|█▍        | 67/469 [00:03<00:22, 17.85it/s]\u001b[A\n",
            "loss=1.8012057542800903 batch_id=68:  15%|█▍        | 69/469 [00:03<00:22, 17.58it/s]\u001b[A\n",
            "loss=1.754625678062439 batch_id=69:  15%|█▍        | 69/469 [00:03<00:22, 17.58it/s] \u001b[A\n",
            "loss=1.636298656463623 batch_id=70:  15%|█▍        | 69/469 [00:03<00:22, 17.58it/s]\u001b[A\n",
            "loss=1.5164742469787598 batch_id=71:  15%|█▍        | 69/469 [00:03<00:22, 17.58it/s]\u001b[A\n",
            "loss=1.5164742469787598 batch_id=71:  15%|█▌        | 72/469 [00:03<00:21, 18.60it/s]\u001b[A\n",
            "loss=1.5885157585144043 batch_id=72:  15%|█▌        | 72/469 [00:03<00:21, 18.60it/s]\u001b[A\n",
            "loss=2.2102248668670654 batch_id=73:  15%|█▌        | 72/469 [00:04<00:21, 18.60it/s]\u001b[A\n",
            "loss=2.2102248668670654 batch_id=73:  16%|█▌        | 74/469 [00:04<00:21, 18.42it/s]\u001b[A\n",
            "loss=2.090628147125244 batch_id=74:  16%|█▌        | 74/469 [00:04<00:21, 18.42it/s] \u001b[A\n",
            "loss=2.222325325012207 batch_id=75:  16%|█▌        | 74/469 [00:04<00:21, 18.42it/s]\u001b[A\n",
            "loss=1.901020884513855 batch_id=76:  16%|█▌        | 74/469 [00:04<00:21, 18.42it/s]\u001b[A\n",
            "loss=1.901020884513855 batch_id=76:  16%|█▋        | 77/469 [00:04<00:20, 19.41it/s]\u001b[A\n",
            "loss=5.647725582122803 batch_id=77:  16%|█▋        | 77/469 [00:04<00:20, 19.41it/s]\u001b[A\n",
            "loss=3.8244338035583496 batch_id=78:  16%|█▋        | 77/469 [00:04<00:20, 19.41it/s]\u001b[A\n",
            "loss=3.8244338035583496 batch_id=78:  17%|█▋        | 79/469 [00:04<00:20, 19.05it/s]\u001b[A\n",
            "loss=2.455033302307129 batch_id=79:  17%|█▋        | 79/469 [00:04<00:20, 19.05it/s] \u001b[A\n",
            "loss=2.4147026538848877 batch_id=80:  17%|█▋        | 79/469 [00:04<00:20, 19.05it/s]\u001b[A\n",
            "loss=2.4147026538848877 batch_id=80:  17%|█▋        | 81/469 [00:04<00:21, 18.47it/s]\u001b[A\n",
            "loss=2.366778612136841 batch_id=81:  17%|█▋        | 81/469 [00:04<00:21, 18.47it/s] \u001b[A\n",
            "loss=2.3332509994506836 batch_id=82:  17%|█▋        | 81/469 [00:04<00:21, 18.47it/s]\u001b[A\n",
            "loss=2.3332509994506836 batch_id=82:  18%|█▊        | 83/469 [00:04<00:20, 18.47it/s]\u001b[A\n",
            "loss=2.3157520294189453 batch_id=83:  18%|█▊        | 83/469 [00:04<00:20, 18.47it/s]\u001b[A\n",
            "loss=2.294262409210205 batch_id=84:  18%|█▊        | 83/469 [00:04<00:20, 18.47it/s] \u001b[A\n",
            "loss=2.294262409210205 batch_id=84:  18%|█▊        | 85/469 [00:04<00:21, 18.21it/s]\u001b[A\n",
            "loss=2.318582534790039 batch_id=85:  18%|█▊        | 85/469 [00:04<00:21, 18.21it/s]\u001b[A\n",
            "loss=2.331775188446045 batch_id=86:  18%|█▊        | 85/469 [00:04<00:21, 18.21it/s]\u001b[A\n",
            "loss=2.331775188446045 batch_id=86:  19%|█▊        | 87/469 [00:04<00:20, 18.54it/s]\u001b[A\n",
            "loss=2.3043625354766846 batch_id=87:  19%|█▊        | 87/469 [00:04<00:20, 18.54it/s]\u001b[A\n",
            "loss=2.2916946411132812 batch_id=88:  19%|█▊        | 87/469 [00:04<00:20, 18.54it/s]\u001b[A\n",
            "loss=2.2916946411132812 batch_id=88:  19%|█▉        | 89/469 [00:04<00:20, 18.16it/s]\u001b[A\n",
            "loss=2.319873809814453 batch_id=89:  19%|█▉        | 89/469 [00:04<00:20, 18.16it/s] \u001b[A\n",
            "loss=2.3535706996917725 batch_id=90:  19%|█▉        | 89/469 [00:04<00:20, 18.16it/s]\u001b[A\n",
            "loss=2.3535706996917725 batch_id=90:  19%|█▉        | 91/469 [00:04<00:20, 18.23it/s]\u001b[A\n",
            "loss=2.334421157836914 batch_id=91:  19%|█▉        | 91/469 [00:05<00:20, 18.23it/s] \u001b[A\n",
            "loss=2.28523588180542 batch_id=92:  19%|█▉        | 91/469 [00:05<00:20, 18.23it/s] \u001b[A\n",
            "loss=2.28523588180542 batch_id=92:  20%|█▉        | 93/469 [00:05<00:20, 18.35it/s]\u001b[A\n",
            "loss=2.2866642475128174 batch_id=93:  20%|█▉        | 93/469 [00:05<00:20, 18.35it/s]\u001b[A\n",
            "loss=2.314023494720459 batch_id=94:  20%|█▉        | 93/469 [00:05<00:20, 18.35it/s] \u001b[A\n",
            "loss=2.314023494720459 batch_id=94:  20%|██        | 95/469 [00:05<00:20, 18.57it/s]\u001b[A\n",
            "loss=2.2990901470184326 batch_id=95:  20%|██        | 95/469 [00:05<00:20, 18.57it/s]\u001b[A\n",
            "loss=2.2866146564483643 batch_id=96:  20%|██        | 95/469 [00:05<00:20, 18.57it/s]\u001b[A\n",
            "loss=2.2866146564483643 batch_id=96:  21%|██        | 97/469 [00:05<00:19, 18.64it/s]\u001b[A\n",
            "loss=2.2907369136810303 batch_id=97:  21%|██        | 97/469 [00:05<00:19, 18.64it/s]\u001b[A\n",
            "loss=2.2689309120178223 batch_id=98:  21%|██        | 97/469 [00:05<00:19, 18.64it/s]\u001b[A\n",
            "loss=2.2689309120178223 batch_id=98:  21%|██        | 99/469 [00:05<00:19, 18.77it/s]\u001b[A\n",
            "loss=2.1995856761932373 batch_id=99:  21%|██        | 99/469 [00:05<00:19, 18.77it/s]\u001b[A\n",
            "loss=2.2836506366729736 batch_id=100:  21%|██        | 99/469 [00:05<00:19, 18.77it/s]\u001b[A\n",
            "loss=2.2801754474639893 batch_id=101:  21%|██        | 99/469 [00:05<00:19, 18.77it/s]\u001b[A\n",
            "loss=2.2801754474639893 batch_id=101:  22%|██▏       | 102/469 [00:05<00:19, 18.88it/s]\u001b[A\n",
            "loss=2.3620798587799072 batch_id=102:  22%|██▏       | 102/469 [00:05<00:19, 18.88it/s]\u001b[A\n",
            "loss=2.321662187576294 batch_id=103:  22%|██▏       | 102/469 [00:05<00:19, 18.88it/s] \u001b[A\n",
            "loss=2.324732780456543 batch_id=104:  22%|██▏       | 102/469 [00:05<00:19, 18.88it/s]\u001b[A\n",
            "loss=2.324732780456543 batch_id=104:  22%|██▏       | 105/469 [00:05<00:18, 19.26it/s]\u001b[A\n",
            "loss=2.3182151317596436 batch_id=105:  22%|██▏       | 105/469 [00:05<00:18, 19.26it/s]\u001b[A\n",
            "loss=2.3408379554748535 batch_id=106:  22%|██▏       | 105/469 [00:05<00:18, 19.26it/s]\u001b[A\n",
            "loss=2.3408379554748535 batch_id=106:  23%|██▎       | 107/469 [00:05<00:19, 18.25it/s]\u001b[A\n",
            "loss=2.317533016204834 batch_id=107:  23%|██▎       | 107/469 [00:05<00:19, 18.25it/s] \u001b[A\n",
            "loss=2.323071241378784 batch_id=108:  23%|██▎       | 107/469 [00:05<00:19, 18.25it/s]\u001b[A\n",
            "loss=2.323071241378784 batch_id=108:  23%|██▎       | 109/469 [00:05<00:19, 18.44it/s]\u001b[A\n",
            "loss=2.29580020904541 batch_id=109:  23%|██▎       | 109/469 [00:05<00:19, 18.44it/s] \u001b[A\n",
            "loss=2.303251266479492 batch_id=110:  23%|██▎       | 109/469 [00:06<00:19, 18.44it/s]\u001b[A\n",
            "loss=2.3046364784240723 batch_id=111:  23%|██▎       | 109/469 [00:06<00:19, 18.44it/s]\u001b[A\n",
            "loss=2.3046364784240723 batch_id=111:  24%|██▍       | 112/469 [00:06<00:19, 18.78it/s]\u001b[A\n",
            "loss=2.304304361343384 batch_id=112:  24%|██▍       | 112/469 [00:06<00:19, 18.78it/s] \u001b[A\n",
            "loss=2.305131196975708 batch_id=113:  24%|██▍       | 112/469 [00:06<00:19, 18.78it/s]\u001b[A\n",
            "loss=2.305131196975708 batch_id=113:  24%|██▍       | 114/469 [00:06<00:18, 18.88it/s]\u001b[A\n",
            "loss=2.3049724102020264 batch_id=114:  24%|██▍       | 114/469 [00:06<00:18, 18.88it/s]\u001b[A\n",
            "loss=2.3008577823638916 batch_id=115:  24%|██▍       | 114/469 [00:06<00:18, 18.88it/s]\u001b[A\n",
            "loss=2.3008577823638916 batch_id=115:  25%|██▍       | 116/469 [00:06<00:18, 18.82it/s]\u001b[A\n",
            "loss=2.2821006774902344 batch_id=116:  25%|██▍       | 116/469 [00:06<00:18, 18.82it/s]\u001b[A\n",
            "loss=2.2789804935455322 batch_id=117:  25%|██▍       | 116/469 [00:06<00:18, 18.82it/s]\u001b[A\n",
            "loss=2.2789804935455322 batch_id=117:  25%|██▌       | 118/469 [00:06<00:20, 17.12it/s]\u001b[A\n",
            "loss=2.2849228382110596 batch_id=118:  25%|██▌       | 118/469 [00:06<00:20, 17.12it/s]\u001b[A\n",
            "loss=2.2970833778381348 batch_id=119:  25%|██▌       | 118/469 [00:06<00:20, 17.12it/s]\u001b[A\n",
            "loss=2.2970833778381348 batch_id=119:  26%|██▌       | 120/469 [00:06<00:19, 17.89it/s]\u001b[A\n",
            "loss=2.2508468627929688 batch_id=120:  26%|██▌       | 120/469 [00:06<00:19, 17.89it/s]\u001b[A\n",
            "loss=2.251268148422241 batch_id=121:  26%|██▌       | 120/469 [00:06<00:19, 17.89it/s] \u001b[A\n",
            "loss=2.251268148422241 batch_id=121:  26%|██▌       | 122/469 [00:06<00:19, 18.08it/s]\u001b[A\n",
            "loss=2.257434368133545 batch_id=122:  26%|██▌       | 122/469 [00:06<00:19, 18.08it/s]\u001b[A\n",
            "loss=2.200265407562256 batch_id=123:  26%|██▌       | 122/469 [00:06<00:19, 18.08it/s]\u001b[A\n",
            "loss=2.200265407562256 batch_id=123:  26%|██▋       | 124/469 [00:06<00:18, 18.55it/s]\u001b[A\n",
            "loss=2.225412130355835 batch_id=124:  26%|██▋       | 124/469 [00:06<00:18, 18.55it/s]\u001b[A\n",
            "loss=2.1650564670562744 batch_id=125:  26%|██▋       | 124/469 [00:06<00:18, 18.55it/s]\u001b[A\n",
            "loss=2.1650564670562744 batch_id=125:  27%|██▋       | 126/469 [00:06<00:19, 17.30it/s]\u001b[A\n",
            "loss=2.195600986480713 batch_id=126:  27%|██▋       | 126/469 [00:06<00:19, 17.30it/s] \u001b[A\n",
            "loss=2.187624454498291 batch_id=127:  27%|██▋       | 126/469 [00:06<00:19, 17.30it/s]\u001b[A\n",
            "loss=2.187624454498291 batch_id=127:  27%|██▋       | 128/469 [00:06<00:19, 17.55it/s]\u001b[A\n",
            "loss=2.1768639087677 batch_id=128:  27%|██▋       | 128/469 [00:07<00:19, 17.55it/s]  \u001b[A\n",
            "loss=2.087986469268799 batch_id=129:  27%|██▋       | 128/469 [00:07<00:19, 17.55it/s]\u001b[A\n",
            "loss=2.147132158279419 batch_id=130:  27%|██▋       | 128/469 [00:07<00:19, 17.55it/s]\u001b[A\n",
            "loss=2.147132158279419 batch_id=130:  28%|██▊       | 131/469 [00:07<00:18, 18.14it/s]\u001b[A\n",
            "loss=1.8642207384109497 batch_id=131:  28%|██▊       | 131/469 [00:07<00:18, 18.14it/s]\u001b[A\n",
            "loss=1.6928640604019165 batch_id=132:  28%|██▊       | 131/469 [00:07<00:18, 18.14it/s]\u001b[A\n",
            "loss=1.6928640604019165 batch_id=132:  28%|██▊       | 133/469 [00:07<00:18, 17.93it/s]\u001b[A\n",
            "loss=1.5857551097869873 batch_id=133:  28%|██▊       | 133/469 [00:07<00:18, 17.93it/s]\u001b[A\n",
            "loss=1.9856916666030884 batch_id=134:  28%|██▊       | 133/469 [00:07<00:18, 17.93it/s]\u001b[A\n",
            "loss=1.9856916666030884 batch_id=134:  29%|██▉       | 135/469 [00:07<00:18, 18.06it/s]\u001b[A\n",
            "loss=2.0797767639160156 batch_id=135:  29%|██▉       | 135/469 [00:07<00:18, 18.06it/s]\u001b[A\n",
            "loss=1.9576821327209473 batch_id=136:  29%|██▉       | 135/469 [00:07<00:18, 18.06it/s]\u001b[A\n",
            "loss=1.9576821327209473 batch_id=136:  29%|██▉       | 137/469 [00:07<00:18, 18.01it/s]\u001b[A\n",
            "loss=2.0735652446746826 batch_id=137:  29%|██▉       | 137/469 [00:07<00:18, 18.01it/s]\u001b[A\n",
            "loss=1.9624663591384888 batch_id=138:  29%|██▉       | 137/469 [00:07<00:18, 18.01it/s]\u001b[A\n",
            "loss=1.9624663591384888 batch_id=138:  30%|██▉       | 139/469 [00:07<00:18, 17.96it/s]\u001b[A\n",
            "loss=1.9234702587127686 batch_id=139:  30%|██▉       | 139/469 [00:07<00:18, 17.96it/s]\u001b[A\n",
            "loss=1.8744317293167114 batch_id=140:  30%|██▉       | 139/469 [00:07<00:18, 17.96it/s]\u001b[A\n",
            "loss=1.8744317293167114 batch_id=140:  30%|███       | 141/469 [00:07<00:18, 17.73it/s]\u001b[A\n",
            "loss=1.8042194843292236 batch_id=141:  30%|███       | 141/469 [00:07<00:18, 17.73it/s]\u001b[A\n",
            "loss=1.4395902156829834 batch_id=142:  30%|███       | 141/469 [00:07<00:18, 17.73it/s]\u001b[A\n",
            "loss=1.4395902156829834 batch_id=142:  30%|███       | 143/469 [00:07<00:18, 17.88it/s]\u001b[A\n",
            "loss=1.3630084991455078 batch_id=143:  30%|███       | 143/469 [00:07<00:18, 17.88it/s]\u001b[A\n",
            "loss=1.3824396133422852 batch_id=144:  30%|███       | 143/469 [00:07<00:18, 17.88it/s]\u001b[A\n",
            "loss=1.3824396133422852 batch_id=144:  31%|███       | 145/469 [00:07<00:19, 16.71it/s]\u001b[A\n",
            "loss=1.3320358991622925 batch_id=145:  31%|███       | 145/469 [00:07<00:19, 16.71it/s]\u001b[A\n",
            "loss=1.3857172727584839 batch_id=146:  31%|███       | 145/469 [00:08<00:19, 16.71it/s]\u001b[A\n",
            "loss=1.3857172727584839 batch_id=146:  31%|███▏      | 147/469 [00:08<00:19, 16.90it/s]\u001b[A\n",
            "loss=1.290871262550354 batch_id=147:  31%|███▏      | 147/469 [00:08<00:19, 16.90it/s] \u001b[A\n",
            "loss=1.0156041383743286 batch_id=148:  31%|███▏      | 147/469 [00:08<00:19, 16.90it/s]\u001b[A\n",
            "loss=1.0156041383743286 batch_id=148:  32%|███▏      | 149/469 [00:08<00:18, 16.85it/s]\u001b[A\n",
            "loss=1.147587537765503 batch_id=149:  32%|███▏      | 149/469 [00:08<00:18, 16.85it/s] \u001b[A\n",
            "loss=1.0564826726913452 batch_id=150:  32%|███▏      | 149/469 [00:08<00:18, 16.85it/s]\u001b[A\n",
            "loss=1.0564826726913452 batch_id=150:  32%|███▏      | 151/469 [00:08<00:19, 16.56it/s]\u001b[A\n",
            "loss=1.1080845594406128 batch_id=151:  32%|███▏      | 151/469 [00:08<00:19, 16.56it/s]\u001b[A\n",
            "loss=0.723644495010376 batch_id=152:  32%|███▏      | 151/469 [00:08<00:19, 16.56it/s] \u001b[A\n",
            "loss=0.723644495010376 batch_id=152:  33%|███▎      | 153/469 [00:08<00:19, 16.50it/s]\u001b[A\n",
            "loss=0.9077821969985962 batch_id=153:  33%|███▎      | 153/469 [00:08<00:19, 16.50it/s]\u001b[A\n",
            "loss=0.9236259460449219 batch_id=154:  33%|███▎      | 153/469 [00:08<00:19, 16.50it/s]\u001b[A\n",
            "loss=0.9236259460449219 batch_id=154:  33%|███▎      | 155/469 [00:08<00:18, 16.66it/s]\u001b[A\n",
            "loss=0.7487089037895203 batch_id=155:  33%|███▎      | 155/469 [00:08<00:18, 16.66it/s]\u001b[A\n",
            "loss=0.7245036363601685 batch_id=156:  33%|███▎      | 155/469 [00:08<00:18, 16.66it/s]\u001b[A\n",
            "loss=0.7245036363601685 batch_id=156:  33%|███▎      | 157/469 [00:08<00:19, 16.10it/s]\u001b[A\n",
            "loss=0.9396225810050964 batch_id=157:  33%|███▎      | 157/469 [00:08<00:19, 16.10it/s]\u001b[A\n",
            "loss=1.113492488861084 batch_id=158:  33%|███▎      | 157/469 [00:08<00:19, 16.10it/s] \u001b[A\n",
            "loss=1.113492488861084 batch_id=158:  34%|███▍      | 159/469 [00:08<00:19, 15.96it/s]\u001b[A\n",
            "loss=0.7385089993476868 batch_id=159:  34%|███▍      | 159/469 [00:08<00:19, 15.96it/s]\u001b[A\n",
            "loss=1.2397925853729248 batch_id=160:  34%|███▍      | 159/469 [00:08<00:19, 15.96it/s]\u001b[A\n",
            "loss=1.2397925853729248 batch_id=160:  34%|███▍      | 161/469 [00:08<00:18, 16.29it/s]\u001b[A\n",
            "loss=0.7054100036621094 batch_id=161:  34%|███▍      | 161/469 [00:08<00:18, 16.29it/s]\u001b[A\n",
            "loss=0.8247520923614502 batch_id=162:  34%|███▍      | 161/469 [00:09<00:18, 16.29it/s]\u001b[A\n",
            "loss=0.8247520923614502 batch_id=162:  35%|███▍      | 163/469 [00:09<00:18, 16.44it/s]\u001b[A\n",
            "loss=0.8896048069000244 batch_id=163:  35%|███▍      | 163/469 [00:09<00:18, 16.44it/s]\u001b[A\n",
            "loss=0.7463099956512451 batch_id=164:  35%|███▍      | 163/469 [00:09<00:18, 16.44it/s]\u001b[A\n",
            "loss=0.7463099956512451 batch_id=164:  35%|███▌      | 165/469 [00:09<00:18, 16.27it/s]\u001b[A\n",
            "loss=0.6301653385162354 batch_id=165:  35%|███▌      | 165/469 [00:09<00:18, 16.27it/s]\u001b[A\n",
            "loss=0.7288652062416077 batch_id=166:  35%|███▌      | 165/469 [00:09<00:18, 16.27it/s]\u001b[A\n",
            "loss=0.7288652062416077 batch_id=166:  36%|███▌      | 167/469 [00:09<00:18, 16.69it/s]\u001b[A\n",
            "loss=0.4476396143436432 batch_id=167:  36%|███▌      | 167/469 [00:09<00:18, 16.69it/s]\u001b[A\n",
            "loss=0.7655221819877625 batch_id=168:  36%|███▌      | 167/469 [00:09<00:18, 16.69it/s]\u001b[A\n",
            "loss=0.7655221819877625 batch_id=168:  36%|███▌      | 169/469 [00:09<00:17, 16.93it/s]\u001b[A\n",
            "loss=0.6254104375839233 batch_id=169:  36%|███▌      | 169/469 [00:09<00:17, 16.93it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d7a4c28d9941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-606770246a31>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34mf'loss={loss.item()} batch_id={batch_idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}